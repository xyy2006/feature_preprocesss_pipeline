Setup and load packages
```{r setup}
source("~/head.r")
require(knitr)
```


Read in example data
```{r readin data}
data_dt <- fread("toy_dataset.csv")

```



```{r QC of columns}
# -------check the column class, if not preferred, convert--------
data_dt[, lapply(.SD, class)] %>% as.data.table 
#%>% knitr::kable(., col.names = "class", caption = "Column class table") %>% message()

#: ------character to POSIXct, this part needs manual, if you find those character type actually need to be POSIXct type.--------
data_dt[ , signup_timestamp := signup_timestamp %>% lubridate::as_datetime()]
data_dt[ , bgc_date := bgc_date %>% lubridate::as_datetime()]
data_dt[ , vehicle_added_date := vehicle_added_date %>% lubridate::as_datetime()]
data_dt[ , first_completed_trip_timestamp := first_completed_trip_timestamp %>% lubridate::as_datetime()]

data_dt[ , days_beween_signup_n_bgc_date := difftime(bgc_date, signup_timestamp, units = "days")]
data_dt[ , days_beween_signup_n_vehicle_added_date := difftime(vehicle_added_date, signup_timestamp, units = "days")]


# #: decide to use the row with most non-NA values for the ids having duplicated rows
# # generate count of non-NA values
data_dt[ , n_nonNA_columns := apply(.SD, 1, function(x) sum(!is.na(x)))]
#: by id, select the one with a greater n_nonNA_columns
data_dt <- data_dt[ order(-n_nonNA_columns) , head(.SD, 1), keyby = id]


#: assign weekdays as a new feature variable,similarly month, quarter, semi-year, year could be extracted.
data_dt[ , signup_proxy_timestamp_weekdays := signup_proxy_timestamp %>% weekdays]

#: check missing data condition----
# check the table
data_dt[, map(.SD, ~sum(is.na(.)))]
```



```{r possible imputation assumption check and see if we need imputation  }
# load imputation package "mice"
library(mice)
library(VIM)
library(caret)
# tabular report of missing
md.pattern(data_dt)

# visualize all features' missingness
aggr_plot <- aggr(data_dt, col=c('navyblue','red'), numbers=TRUE, sortVars=TRUE, labels=names(data_dt), cex.axis=.7, gap=3, ylab=c("Histogram of missing data","Pattern"))

#: conclusion: 44% missing data ----


# ------imputation could be done within autoSklearn feature preprocess module or using R mice----
# mice func has many imputation methods to select as arguments.
###::: start multivariate impute using mice func.
# data_to_impute <- training_selectedCols[, .(transvalue_previous, timediff_currentMinusLast, bathroomcnt ,bedroomcnt ,rank_school_score, finishedsquarefeet, lotsizesquarefeet, median_house_value_censusTract, median_household_income_censusTract, viewtypeid, builtyear,storycnt,water_land_ratio, pop_growth_rate, education_index, crime_index  )] 
#---#
# 1st batch impute, if too many, will singular value.
data_to_impute <- training_selectedCols[, .(transvalue, transvalue_previous, timediff_currentMinusLast, bathroomcnt ,bedroomcnt ,rank_school_score, finishedsquarefeet, lotsizesquarefeet, median_house_value_censusTract, median_household_income_censusTract )] 
# use mice function
X_imputed <- mice(data = data_to_impute)
data_after_impute <- complete(X_imputed,5)
#
# append to training_selectedCols
training_selectedCols[, colnames(data_to_impute) := data_after_impute]
#---#
# 2nd batch impute, for builtyear
data_to_impute <- training_selectedCols[, .(transvalue, transvalue_previous,timediff_currentMinusLast, bathroomcnt ,bedroomcnt ,finishedsquarefeet, lotsizesquarefeet, builtyear )] 
# use mice function
X_imputed <- mice(data = data_to_impute)
data_after_impute <- complete(X_imputed,5)
# append to training_selectedCols
training_selectedCols[, colnames(data_after_impute) := data_after_impute]
#---#
# 3rd batch impute, for storycnt
data_to_impute <- training_selectedCols[, .(transvalue,transvalue_previous,timediff_currentMinusLast, bathroomcnt ,bedroomcnt ,finishedsquarefeet, lotsizesquarefeet, storycnt )] 
# use mice function
X_imputed <- mice(data = data_to_impute)
data_after_impute <- complete(X_imputed,5)
# append to training_selectedCols
training_selectedCols[, colnames(data_after_impute) := data_after_impute]
#---#
# 4th batch impute, for many location based property.
data_to_impute <- training_selectedCols[, .(rank_school_score, median_house_value_censusTract, median_household_income_censusTract, water_land_ratio, pop_growth_rate, education_index, crime_index, land_area, pop_density_censusTract, housing_units_censusTract, water_area )] 
# use mice function
X_imputed <- mice(data = data_to_impute)
data_after_impute <- complete(X_imputed,5)
# append to training_selectedCols
training_selectedCols[, colnames(data_after_impute) := data_after_impute]
#---#
# 4th batch impute, for census_tract.
data_to_impute <- training_selectedCols[, .(rank_school_score, median_house_value_censusTract, median_household_income_censusTract, water_land_ratio, pop_growth_rate, education_index, crime_index, land_area, pop_density_censusTract, housing_units_censusTract, water_area, census_tract )] 
# use mice function
X_imputed <- mice(data = data_to_impute, method = "lda")
data_after_impute <- complete(X_imputed,5)
# append to training_selectedCols
training_selectedCols[, colnames(data_after_impute) := data_after_impute]
###::: finish imputing in training_selectedCols

#:
#
##: convert logical to numeric for later modeling purpose----
# (algorithm can be approved, overhead in [.data.table still exists in `set(..., value = xxxxxxxx)` part)
logical_class_index <- data_dt %>% sapply(., function(x) is(x, "logical")) %>% which
message("Logical class features have: ", logical_class_index %>% length)
noReturn <- foreach( index = logical_class_index) %do% {
	message("convert logical to integer on index: ", index)
	set(data_dt, j = index, value = data_dt[, index, with = FALSE] %>% unlist %>% as.numeric )
	return(NULL)
}



#: convert some character to factor/integer, such as gender.----
char_colnames <- names(data_dt)[ data_dt %>% map_chr(., class) == "character"]

noReturn <- foreach( cn = char_colnames) %do% {
# foreach( cn = c("Gender", "StateAgg")) %do% {
	data_dt[ , (cn) := eval(parse(text = cn)) %>% as.factor]
	invisible(NULL)
}

#---now filter on var == 0 on all non-character columns----------
non_character_index <- data_dt %>% sapply(., function(x) !is(x, "character")) %>% which
message("Non character features have: ", non_character_index %>% length)
var_vec <- data_dt[, sapply(.SD, var, na.rm = TRUE), .SDcols = non_character_index]
#: update these cols to NULL (i.e., remove these cols from dt) 
data_dt[ , names(var_vec)[which(var_vec == 0)] := NULL]

#---now filter on nearZeroVar on all non-character columns----------
non_character_index <- data_dt %>% sapply(., function(x) !is(x, "character")) %>% which
message("Non character features have: ", non_character_index %>% length)
colname_lowvar <- nearZeroVar(data_dt[, non_character_index, with = FALSE], freqCut = 95/5, uniqueCut = 10, names = TRUE, foreach = FALSE, allowParallel = TRUE) 
#
endpoint_col_index <- grep("flag", colname_lowvar, value = FALSE, ignore = TRUE)
if (length(endpoint_col_index) > 0) {
	colname_lowvar <- colname_lowvar[ -endpoint_col_index]
}
#: update these cols to NULL (i.e., remove these cols from dt) 
data_dt[ , (colname_lowvar) := NULL]

##--- now filter on findCorrelation on all integer|numeric columns----------
integer_or_numeric_index <- data_dt %>% sapply(., function(x) is(x, "integer")|is(x, "numeric")) %>% which
# remove endpoint from integer_or_numeric_index
endpoint_col_index <- grep("flag", integer_or_numeric_index %>% names, value = FALSE, ignore = TRUE)
if (length(endpoint_col_index) > 0) {
	integer_or_numeric_nonEndpoint_index <- integer_or_numeric_index[ -endpoint_col_index]
} else {
  integer_or_numeric_nonEndpoint_index <- integer_or_numeric_index
}

message("Integer|Numeric non-endpoint features have: ", integer_or_numeric_nonEndpoint_index %>% length)
if (length(integer_or_numeric_nonEndpoint_index) > 1) {
  colname_highCorr <- data_dt[ , integer_or_numeric_nonEndpoint_index, with = FALSE] %>% cor %>% findCorrelation(., .9, names = TRUE, exact = TRUE, verbose = TRUE)
  data_dt[, (colname_highCorr) := NULL]
}



#------- merge too many levels categorical variables-------------------
#: define the function of merging levels in a categorical variable
#' Title: merge levels with insufficient sample size to a level names as "others".
#'
#' @param categorical_variable_name a string. the variable name in your dt
#' @param dt a data.table object
#' @param first_n an integer. specifiy how many biggest levels we should pre-exclude in later calculation. If first_n = 3,remove first 3 biggest large group from denominator, and also remove the sum of their combined sample size from numerator. Then calculate averge group size should be. 
#' @param missing_value the extra missing value deemed as missing value in the variable/column. default "". Note NA or NaN are are default missing values (will be counted as different levels by function `table`).
#' @param minimal_sample_size_per_level if use this, `first_n` based method will be ignored. Levels with less than `minimal_sample_size_per_level` sample size will be merged to `merged_value`
#' @param merged_value the character value you want the merged levels to be.
#'
#' @return a variable contains the merged levels and those intact levels from original `categorical_variable_name`.
#' @export
#'
#' @examples
#' new_variable_value <- merge_trivialLevels_in_categoricalVariable(variable_name, dt = data_dt, first_n = 2, missing_value = "", merged_value = "others")
#' data_dt[ , (variable_name) := new_variable_value]

merge_trivialLevels_in_categoricalVariable <- function(categorical_variable_name, dt, first_n = 1, missing_value = "", flag_use_minimal_sample_size_per_level = FALSE, minimal_sample_size_per_level = 20, merged_value = "others"){
 
  if (data_dt[ , categorical_variable_name, with = FALSE] %>% unlist %>% class == "factor") {
    data_dt[ , (categorical_variable_name) := eval(as.name(categorical_variable_name)) %>% as.character()]
  }
  sample_size_by_group <-  dt[, categorical_variable_name, with = FALSE] %>% table(., useNA = "always")
  
  if (!flag_use_minimal_sample_size_per_level) {
    samplesize_average_by_groups <- (nrow(dt) - head(sort(sample_size_by_group, decreasing = TRUE), first_n) %>% sum) / (dt[, categorical_variable_name, with = FALSE] %>% n_distinct  - first_n)
    names_groups_with_smaller_samplesize <- names(sample_size_by_group)[sample_size_by_group < samplesize_average_by_groups]
  } else {
    names_groups_with_smaller_samplesize <- names(sample_size_by_group)[sample_size_by_group < minimal_sample_size_per_level]
  }
  original_variable_value <- dt[ , categorical_variable_name, with = FALSE] %>% unlist(., use.names = FALSE)
  original_variable_value[original_variable_value %in% c(names_groups_with_smaller_samplesize, missing_value)] <- merged_value
  return(original_variable_value %>% as.factor)
}

##: could loop over and automatic for levels merge for all factor/character variables.
character_or_factor_index <- data_dt %>% sapply(., function(x) is(x, "factor")|is(x, "character")) %>% which
#
temp <- foreach (variable_name = c("vehicle_name", "vehicle_make")) %do% {
  new_variable_value <- merge_trivialLevels_in_categoricalVariable(variable_name, dt = data_dt, first_n = 2, missing_value = "", merged_value = "others")
  data_dt[ , (variable_name) := new_variable_value]
}

# check
# temp %>% map(., ~table(.) %>% sort)
```




```{r setup parallel backend, eval=FALSE, echo = TRUE}
# install.packages("doParallel")

library(doParallel)

ncpu_to_use <- detectCores() - 2

cl <- makePSOCKcluster(ncpu_to_use)
registerDoParallel(cl)

getDoParRegistered()
getDoParName()
getDoParWorkers()
```


This part could be automated by autoSklearn or autoCaret.
```{r ML model training, validation and testing, eval=FALSE, echo = TRUE}
library(caret)
#: create another version of data_dt with all complete feature obs.----
data_dt_noMissing <- data_dt[ signup_os == "missing info", ]
nrow(data_dt_noMissing)
# then we can do sensitivity analysis by replace data_dt with data_dt_noMissing below.


#: data splitting to two parts,----
# training + validation part, leave it for cross-validation technicque
# testing part
#: match outcome ratio splitting
set.seed(9420)
trainIndex <- createDataPartition(data_dt$flag_drive_within_30days, p = .7, list = FALSE )
nrow(trainIndex)

data_dt_trainModel_part <- data_dt[ trainIndex, ]
nrow(data_dt)
nrow(data_dt_trainModel_part)

data_dt_testingModel_part <- data_dt[ -trainIndex,]
nrow(data_dt_testingModel_part)
#: ML model - e.g., use xgbTree model----
# eXtreme Gradient Boosting
# 
#   method = 'xgbTree'
# Type: Regression, Classification
# 
# Tuning parameters:
# 
# nrounds (# Boosting Iterations)
# max_depth (Max Tree Depth)
# eta (Shrinkage)
# gamma (Minimum Loss Reduction)
# colsample_bytree (Subsample Ratio of Columns)
# min_child_weight (Minimum Sum of Instance Weight)
# subsample (Subsample Percentage)


# xgbT_Grid <-  expand.grid(interaction.depth = c(1, 5, 9), 
#                         n.trees = (1:30)*50, 
#                         shrinkage = 0.1,
#                         n.minobsinnode = 20)

#: I tried both grid search as default in `trainContrl` and the random search of a few parameter combinations.
fitControl <- trainControl(method = "repeatedcv", number = 10, repeats = 1, classProbs = TRUE, summaryFunction = twoClassSummary)
# fitControl <- trainControl(method = "repeatedcv", number = 10, repeats = 1, classProbs = TRUE, summaryFunction = twoClassSummary, search = "random")
# #
# data_dt_trainModel_part[, signup_os := gsub("\\s", "_", signup_os)]
xgbTree_fit1 <- train( flag_drive_within_30days_validName ~ ., data = data_dt_trainModel_part[, -"id", with = FALSE], metric = "ROC", method = "xgbTree", trControl = fitControl)

xgbTree_fit2 <- train( flag_drive_within_30days_validName ~ ., data = data_dt_trainModel_part[, -c("id", "vehicle_name"), with = FALSE], metric = "ROC", method = "xgbTree", trControl = fitControl)

# avNBC_fit1 <- train( flag_drive_within_30days_validName ~ ., data = data_dt_trainModel_part[, -c("id", "vehicle_name"), with = FALSE], metric = "ROC", method = "manb", trControl = fitControl)

avNNet_fit1 <- train( flag_drive_within_30days_validName ~ ., data = data_dt_trainModel_part[, -c("id" ), with = FALSE], metric = "ROC", method = "avNNet", trControl = fitControl)

avNNet_fit2 <- train( flag_drive_within_30days_validName ~ ., data = data_dt_trainModel_part[, -c("id", "vehicle_name"), with = FALSE], metric = "ROC", method = "avNNet", trControl = fitControl)


# avBayLasso_fit1 <- train( flag_drive_within_30days_validName ~ ., data = data_dt_trainModel_part[, -c("id", "vehicle_name"), with = FALSE], metric = "ROC", method = "blassoAveraged", trControl = fitControl)

h2o::h2o.init()
glmnet_h2o_fit1 <- train( flag_drive_within_30days_validName ~ ., data = data_dt_trainModel_part[, -c("id", "vehicle_name"), with = FALSE], metric = "ROC", method = "glmnet_h2o", trControl = fitControl)
h2o::h2o.shutdown(prompt = FALSE)

# (I tested the effect of vehicle_age^2 and categorize vehicle_age to factor respectively after all the work done. )
glmnet_fit_with_agesq <- train( flag_drive_within_30days_validName ~ . + I(vehicle_age^2), data = data_dt_trainModel_part[, -c("id"), with = FALSE], metric = "ROC", method = "glmnet", trControl = fitControl)
glmnet_fit_with_ageAsCateg <- train( flag_drive_within_30days_validName ~ . + as.factor(vehicle_age), data = data_dt_trainModel_part[, -c("id"), with = FALSE], metric = "ROC", method = "glmnet", trControl = fitControl)
# generate binned vehicle_age variable in data_dt_trainModel_part
data_dt_trainModel_part[, vehicle_age_binned := cut(vehicle_age, breaks = c(-1, 5, 10, 15, 20, max(data_dt$vehicle_age)))]
# # extract cutting points at right side of each bin.
# cut_points_vec <- data_dt_trainModel_part$vehicle_age_binned %>% levels() %>% str_match_all(., "(\\d+)\\]") %>% map_dbl(., ~.[1,2] %>% as.numeric)
# do the same thing in testing dataset.
data_dt_testingModel_part[, vehicle_age_binned := cut(vehicle_age, breaks =  c(-1, 5, 10, 15, 20, max(data_dt$vehicle_age)))]
# train with binned variable
glmnet_fit_with_ageAsBinedCateg <- train( flag_drive_within_30days_validName ~ . + vehicle_age_binned, data = data_dt_trainModel_part[, -c("id"), with = FALSE], metric = "ROC", method = "glmnet", trControl = fitControl)

# regular glmnet fit like other methods design
glmnet_fit1 <- train( flag_drive_within_30days_validName ~ ., data = data_dt_trainModel_part[, -c("id"), with = FALSE], metric = "ROC", method = "glmnet", trControl = fitControl)
glmnet_fit2 <- train( flag_drive_within_30days_validName ~ ., data = data_dt_trainModel_part[, -c("id", "vehicle_name"), with = FALSE], metric = "ROC", method = "glmnet", trControl = fitControl)
#: (dont run) Too computation intensive, will crash your laptop.
# avGlm_fit1 <- train( flag_drive_within_30days_validName ~ ., data = data_dt_trainModel_part[, -c("id"), with = FALSE], metric = "ROC", method = "randomGLM", trControl = fitControl)
# avGlm_fit2 <- train( flag_drive_within_30days_validName ~ ., data = data_dt_trainModel_part[, -c("id", "vehicle_name"), with = FALSE], metric = "ROC", method = "randomGLM", trControl = fitControl)
#
dnn_fit1 <- train( flag_drive_within_30days_validName ~ ., data = data_dt_trainModel_part[, -c("id" ), with = FALSE], metric = "ROC", method = "dnn", trControl = fitControl)
dnn_fit2 <- train( flag_drive_within_30days_validName ~ ., data = data_dt_trainModel_part[, -c("id", "vehicle_name"), with = FALSE], metric = "ROC", method = "dnn", trControl = fitControl)



# deepBoost_fit1 <- train( flag_drive_within_30days_validName ~ ., data = data_dt_trainModel_part[, -c("id", "vehicle_name"), with = FALSE], metric = "ROC", method = "deepboost", trControl = fitControl)

#
bayesglm_fit1 <- train( flag_drive_within_30days_validName ~ ., data = data_dt_trainModel_part[, -c("id"), with = FALSE], metric = "ROC", method = "bayesglm", trControl = fitControl)
bayesglm_fit2 <- train( flag_drive_within_30days_validName ~ ., data = data_dt_trainModel_part[, -c("id", "vehicle_name"), with = FALSE], metric = "ROC", method = "bayesglm", trControl = fitControl)
#
svm_fit1 <- train( flag_drive_within_30days_validName ~ ., data = data_dt_trainModel_part[, -c("id"  ), with = FALSE], metric = "ROC", method = "svmRadial", trControl = fitControl)
svm_fit2 <- train( flag_drive_within_30days_validName ~ ., data = data_dt_trainModel_part[, -c("id", "vehicle_name"  ), with = FALSE], metric = "ROC", method = "svmRadial", trControl = fitControl)
```


***

#### Model benchmark result report
```{r models benchmark}
#: ----show some plots of some methods to see how tuning the parameter affects the performance----
#: plot parameter tuning effect on metric performance----
trellis.par.set(caretTheme())
plot(xgbTree_fit1)  
#: extract final model, allow 2% loss of performance but prefer a more simplistic model.
whichTwoPct <- tolerance(xgbTree_fit1$results, metric = "ROC", 
                         tol = 2, maximize = TRUE)  
message("best model within 2 pct of best:")
xgbTree_fit1$results[whichTwoPct, ]
# a max_depth gamma colsample_bytree min_child_weight subsample
# 1 0.3         1     0              0.6                1       0.5

#: plot parameter tuning effect on metric performance----
trellis.par.set(caretTheme())
plot(xgbTree_fit2)  
#: extract final model, allow 2% loss of performance but prefer a more simplistic model.
whichTwoPct <- tolerance(xgbTree_fit2$results, metric = "ROC", 
                         tol = 2, maximize = TRUE)  
message("best model within 2 pct of best:")
xgbTree_fit2$results[whichTwoPct, ]
# a max_depth gamma colsample_bytree min_child_weight subsample
# 1 0.3         1     0              0.6                1       0.5

#: plot parameter tuning effect on metric performance----
trellis.par.set(caretTheme())
plot(avNNet_fit1)  
#: extract final model, allow 2% loss of performance but prefer a more simplistic model.
whichTwoPct <- tolerance(avNNet_fit1$results, metric = "ROC", 
                         tol = 2, maximize = TRUE)  
message("best model within 2 pct of best:")
avNNet_fit1$results[whichTwoPct, ]
# a max_depth gamma colsample_bytree min_child_weight subsample
# 1 0.3         1     0              0.6                1       0.5

#: plot parameter tuning effect on metric performance----
trellis.par.set(caretTheme())
plot(dnn_fit2)  
#: extract final model, allow 2% loss of performance but prefer a more simplistic model.
whichTwoPct <- tolerance(dnn_fit2$results, metric = "ROC", 
                         tol = 2, maximize = TRUE)  
message("best model within 2 pct of best:")
dnn_fit2$results[whichTwoPct, ]
# a max_depth gamma colsample_bytree min_child_weight subsample
# 1 0.3         1     0              0.6                1       0.5



#:----between-model performance summary----
resamps <- resamples(list(dnn_fit1 = dnn_fit1,
                          dnn_fit2 = dnn_fit2,
                          xgbTree_fit1 = xgbTree_fit1,
                          xgbTree_fit2 = xgbTree_fit2,
                          avNNet_fit1 = avNNet_fit1,
                          avNNet_fit2 = avNNet_fit2,
                          glmnet_h2o_fit1 = glmnet_h2o_fit1,
                          glmnet_fit1 = glmnet_fit1,
                          glmnet_fit2 = glmnet_fit2,
                          glmnet_fit_with_agesq = glmnet_fit_with_agesq,
                          glmnet_fit_with_ageAsCateg = glmnet_fit_with_ageAsCateg,
                          glmnet_fit_with_ageAsBinedCateg = glmnet_fit_with_ageAsBinedCateg,
                          bayesglm_fit1 = bayesglm_fit1,                          
                          bayesglm_fit2 = bayesglm_fit2,                          
                          svm_fit1 = svm_fit1,
                          svm_fit2 = svm_fit2
                          )) 

resamps
summary(resamps)

#: visualize it
theme1 <- trellis.par.get()
theme1$plot.symbol$col = rgb(.2, .2, .2, .4)
theme1$plot.symbol$pch = 16
theme1$plot.line$col = rgb(1, 0, 0, .7)
theme1$plot.line$lwd <- 2
trellis.par.set(theme1)
#
bwplot(resamps, layout = c(3, 1))
#
trellis.par.set(caretTheme())
dotplot(resamps, metric = "ROC")
```


#### Predict on testing dataset (hold-out) and summarize the performance
The above training and validation were implemented within the 70% of the dataset using 10-fold CV. Ideally, the testing error should be obtained with the 30% hold-out testing dataset.

```{r predict on testing dataset (hold-out) and summarize the performance}
library(ROCR)

performance_measures_dt <- foreach( model_fitted = list(dnn_fit1 = dnn_fit1,
                                                        dnn_fit2 = dnn_fit2,
                                                        xgbTree_fit1 = xgbTree_fit1,
                                                        xgbTree_fit2 = xgbTree_fit2,
                                                        avNNet_fit1 = avNNet_fit1,
                                                        avNNet_fit2 = avNNet_fit2,
                                                        glmnet_fit1 = glmnet_fit1,
                                                        glmnet_fit2 = glmnet_fit2,
                                                        glmnet_fit_with_agesq = glmnet_fit_with_agesq,
                                                        glmnet_fit_with_ageAsCateg = glmnet_fit_with_ageAsCateg,
                                                        glmnet_fit_with_ageAsBinedCateg = glmnet_fit_with_ageAsBinedCateg,
                                                        bayesglm_fit1 = bayesglm_fit1,                          
                                                        bayesglm_fit2 = bayesglm_fit2,                          
                                                        svm_fit1 = svm_fit1,
                                                        svm_fit2 = svm_fit2 ), .combine = rbind ) %do% {
    # predictions <- predict.glm(model_fitted, newdata = data_test, type = "response" )
    predictions <- predict(model_fitted, newdata = data_dt_testingModel_part, type = "prob" ) %>% .["yes"] %>% unlist
    #predictions <- predictions >= 0.5 # for debug purpose
    #pred <- prediction(predictions %>% as.integer, labels = data_dt[ , endpoint] %>% as.integer)
    pred <- prediction(predictions %>% as.numeric, labels = data_dt_testingModel_part$flag_drive_within_30days_validName)
    perf_auc <- performance(pred, "auc")
    perf_mat <- performance(pred,  "mat")
    perf_prec <- performance(pred,  "prec", "rec")
    print(perf_auc@y.values[[1]])
    #print(perf_mat@y.values[[1]])
    mcc_value_max <- perf_mat@y.values %>% unlist %>% max(., na.rm = TRUE)
    print(mcc_value_max)
    
    x_cutoff_at_max_mcc <- perf_mat@x.values %>% unlist %>% .[which.max(perf_mat@y.values %>% unlist)]
    print(x_cutoff_at_max_mcc)
    
    prec_at_specific_recall <- perf_prec@y.values %>% unlist %>% .[which(unlist(perf_prec@x.values) >= .8) %>% min] # here at least .8
    message("Precision at at least .8 recall is ", prec_at_specific_recall)
                                        
    #
    return(c(AUC = perf_auc@y.values[[1]], MCC = mcc_value_max, best_cutoff_at_max_mcc = x_cutoff_at_max_mcc, prec_at_ge_80perc_recall = prec_at_specific_recall))
    
  }
rownames(performance_measures_dt) <- list(dnn_fit1 = dnn_fit1,
                                          dnn_fit2 = dnn_fit2,
                                          xgbTree_fit1 = xgbTree_fit1,
                                          xgbTree_fit2 = xgbTree_fit2,
                                          avNNet_fit1 = avNNet_fit1,
                                          avNNet_fit2 = avNNet_fit2,
                                          glmnet_fit1 = glmnet_fit1,
                                          glmnet_fit2 = glmnet_fit2,
                                          glmnet_fit_with_agesq = glmnet_fit_with_agesq,
                                          glmnet_fit_with_ageAsCateg = glmnet_fit_with_ageAsCateg,
                                          glmnet_fit_with_ageAsBinedCateg = glmnet_fit_with_ageAsBinedCateg,
                                          bayesglm_fit1 = bayesglm_fit1,                          
                                          bayesglm_fit2 = bayesglm_fit2,                          
                                          svm_fit1 = svm_fit1,
                                          svm_fit2 = svm_fit2 ) %>% names
performance_measures_dt <- performance_measures_dt %>% data.table(., keep.rownames = TRUE)

performance_measures_dt %>% arrange(., -AUC)

```

